<!DOCTYPE html>
<html>

<head>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            background-color: #f5f5f5;
        }

        a {
            color: #4183C4;
            text-decoration: none;
        }

        p {
            line-height: 20px;
        }

        .content {
            max-width: 800px;
            margin: auto;
        }

        #abs {
            text-align: center;
        }

        #abs .descriptor {
            display: none;
        }

        #abs h1.title {
            margin: .5em 0 .5em 20px;
            font-size: x-large;
            font-weight: bold;
            line-height: 120%;
        }

        #abs .authors {
            margin: .5em 0 .5em 20px;
            font-size: medium;
            line-height: 150%;
        }

        #abs .authors a {
            font-size: medium;
        }

        #abs p {
            text-align: justify;
        }

        .bib {
            font-size: small;
        }

        .figure {
            text-align: center;
        }
    </style>
</head>
<body>
<div class="content">
    <div id="abs">
        <h1>Learning Multimodal Latent Generative Models with Energy-Based Prior</h1>
        <div class="authors"><a href="mailto:syuan14@stevens.edu">Shiyu Yuan</a>,  Jiali Cui,  Hanao Li, Tian Han</div>
        <div class="inst">
            Stevens Institute of Technology, USA<br>
        </div>
        <h2>Abstract</h2>
        <p>Multimodal models have gained increasing popularity recently. Many works have been proposed to learn the representations for different modalities. The representation can learn shared information from these domains, leading to increased and coherent joint and cross-generation. However, these works mainly considered standard Gaussian or Laplacian as their prior distribution. It can be challenging for the uni-modal and non-informative distribution to capture all the information from multiple data types. Meanwhile, energy-based models (EBM) have shown their effectiveness in multiple tasks due to their expressiveness and flexibility. But its capacity has yet to be discovered for the multimodal generative models. In this paper, we propose a novel framework to train multimodal latent generative models together with the energy-based models. The proposed method can lead to more expressive and informative prior which can better capture the information within multiple modalities. Our experiments showed that our model is effective and can increase generation coherence and latent classification for different multimodal datasets.</p>
    </div>
    <hr>
    <h2>Paper</h2>
    The publication can be obtained <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/12427.pdf">here</a>.
    <pre class="bib">
@article{syuan2024mulener,
  title={Learning Multimodal Latent Generative Models with Energy-Based Prior},
  author={Shiyu, Yuan and Jiali, Cui and Hanao, Li and Tian, Han},
  journal={ECCV},
  year={2024}
}
</pre>
    <hr>

    <h2>Contributions</h2>
    <p>(1)We propose the energy-based prior model for multimodal latent generative models to capture complex shared information within multiple modalities.<br>
(2) We develop the variational training scheme where the generation model, inference model, and energy-based prior can be jointly and effectively learned.<br>
(3) We conduct various experiments and ablation studies and demonstrate superior performance compared to Laplacian prior baselines.</p>
    <hr>
    

    <h2>Code</h2>
    <p>The code can be obtained <a href="https://github.com/syyuan2021/Learning-Multimodal-Latent-Generative-Models-with-EBM">here</a>. </p>
    <hr>
    
    <h2>Experiments</h2>
    <h3>Experiment 1: Image</h3>
    <p>The generator network \(p_\theta\) in our framework is well-learned to generate samples that are realistic and share visual similarities as the training data. The qualitative results are shown in Figure 1. We further evaluate our model quantitatively by using Fr√©chet Inception Distance (FID) in the table below. It can be seen that our model achieves superior generation performance compared to listed baseline models.</p>
    <div class="figure">
        <img src="./figure/image/svhn/synthesis/synthesis_latent_ebm.png" width="200" style="margin:10px">
        <img src="./figure/image/cifar10/synthesis/cifar10_syn.png" width="200" style="margin:10px">
        <img src="./figure/image/celeba/synthesis/celeba_latent_ebm_synthesis.png" width="200" style="margin:10px">
        <p class="caption"><b>Figure 1:</b> Generated samples for SVHN \((32 \times 32)\), CIFAR-10 \((32 \times 32)\), and CelebA \((64 \times 64)\).</p>
    </div>


    <div class="figure">
        <img src="./figure/image/fid/fid_table.png" width="800" style="margin:10px">
    </div>


    <h3>Experiment 2: Text</h3>
    <p> To evaluate the quality of the generated samples, we recruit Forward Perplexity (FPPL) and Reverse Perplexity (RPPL). FPPL is the perplexity of the generated samples evaluated under a language model trained with real data and measures the fluency of the synthesized sentences. RPPL is the perplexity of real data (the test data partition) computed under a language model trained with the model-generated samples. Prior work employs it to measure the distributional coverage of a learned model, \(p_\theta(x)\) in our case, since a model with a mode-collapsing issue results in a high RPPL. FPPL and RPPL are displayed in Table 2. Our model outperforms all the baselines on the two metrics, demonstrating the high fluency and diversity of the samples from our model. </p>
    <div class="figure">
        <img src="./figure/text/text_table.png" width="800" style="margin:10px">
    </div>


    <h3>Experiment 3: Analysis of latent space</h3>
    <p> We examine the exponential tilting of the reference prior \(p_0(z)\) through Langevin samples initialized from \(p_0(z)\) with target distribution \(p_\alpha(z)\). As the reference distribution \(p_0(z)\) is in the form of an isotropic Gaussian, we expect the energy-based correction \(f_\alpha\) to tilt \(p_0\) into an irregular shape like some shallow local modes. Therefore, the trajectory of a Markov chain initialized from the reference distribution \(p_0(z)\) with well-learned target \(p_\alpha(z)\) should depict the transition towards synthesized examples of high quality while the energy fluctuates around some constant. Figure 2 depicts such transitions for CelebA, which is based on a model trained with \(K_0 = 40\) steps. The quality of synthesis improves significantly with increasing number of steps.
 </p>
    <div class="figure">
        <img src="./figure/transition/image_transition.png" width="800" style="margin:10px">
        <p class="caption"><b>Figure 2:</b> Transition of Markov chains initialized from \(p_0(z)\) towards \(\tilde{p}_{\alpha}(z)\) for \(K_0'=100\) steps. Top: Trajectory in the CelebA data-space. Bottom: Energy profile over time.</p>
    </div>


    <h3>Experiment 4: Anomaly detection</h3>
    <p> If the generator and EBM are well learned, then the posterior \(p_\theta(z|x)\) would form a discriminative latent space that has separated probability densities for normal and anomalous data. Samples from such latent space can then be used as discriminative features to detect anomalies. We perform posterior sampling on the learned model to obtain the latent samples, and use the unnormalized log-posterior \(\log p_\theta(x, z)\) as our decision function. </p>
    <div class="figure">
        <img src="./figure/anomaly/anomaly_table.png" width="800" style="margin:10px">
    </div>


    <h3>Experiment 5: Scalability</h3>
    <p> We have also explored avenues to improve training speed and found that a PyTorch extension, NVIDIA Apex, is able to improve our model training 2.5 times. We test our method with Apex training on a larger scale dataset, CelebA \( (128 \times 128 \times 3) \). The learned model is able to synthesize examples with high fidelity. </p>
    <div class="figure">
        <img src="./figure/image/celeba_large/celeba128_synthesis.png" width="600" style="margin:10px">
    </div>


    <hr>
    <h2>Acknowledgements</h2>
    <p>This work is supported in part by NSF IIS-2339604. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.</p>
</div>
</body>
</html>
