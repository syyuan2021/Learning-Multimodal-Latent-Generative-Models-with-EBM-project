<!DOCTYPE html>
<html>

<head>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            background-color: #f5f5f5;
        }

        a {
            color: #4183C4;
            text-decoration: none;
        }

        p {
            line-height: 20px;
        }

        .content {
            max-width: 800px;
            margin: auto;
        }

        #abs {
            text-align: center;
        }

        #abs .descriptor {
            display: none;
        }

        #abs h1.title {
            margin: .5em 0 .5em 20px;
            font-size: x-large;
            font-weight: bold;
            line-height: 120%;
        }

        #abs .authors {
            margin: .5em 0 .5em 20px;
            font-size: medium;
            line-height: 150%;
        }

        #abs .authors a {
            font-size: medium;
        }

        #abs p {
            text-align: justify;
        }

        .bib {
            font-size: small;
        }

        .figure {
            text-align: center;
        }
    </style>
</head>
<body>
<div class="content">
    <div id="abs">
        
        <h1>Learning Multimodal Latent Generative Models with Energy-Based Prior</h1>
        <p style="text-align: center;">ECCV2024 Oral</p>
        <div class="authors">
            <a href="mailto:syuan14@stevens.edu">Shiyu Yuan</a> (Department of Systems Engineering),  
            Jiali Cui, Hanao Li, Tian Han (Department of Computer Science)
        </div>
        <div class="inst">
            Stevens Institute of Technology, USA<br>
        </div>

        <h2>Abstract</h2>
        <p>Multimodal models have gained increasing popularity recently. Many works have been proposed to learn the representations for different modalities. The representation can learn shared information from these domains, leading to increased and coherent joint and cross-generation. However, these works mainly considered standard Gaussian or Laplacian as their prior distribution. It can be challenging for the uni-modal and non-informative distribution to capture all the information from multiple data types. Meanwhile, energy-based models (EBM) have shown their effectiveness in multiple tasks due to their expressiveness and flexibility. But its capacity has yet to be discovered for the multimodal generative models. In this paper, we propose a novel framework to train multimodal latent generative models together with the energy-based models. The proposed method can lead to more expressive and informative prior which can better capture the information within multiple modalities. Our experiments showed that our model is effective and can increase generation coherence and latent classification for different multimodal datasets.</p>
    </div>
    <hr>
    <h2>Paper</h2>
    The publication can be obtained <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/12427.pdf">here</a>.
    <pre class="bib">
@article{syuan2024mulener,
  title={Learning Multimodal Latent Generative Models with Energy-Based Prior},
  author={Shiyu, Yuan and Jiali, Cui and Hanao, Li and Tian, Han},
  journal={ECCV},
  year={2024}
}
</pre>
    <hr>

    <h2>Contributions</h2>
    <p>(1)We propose the energy-based prior model for multimodal latent generative models to capture complex shared information within multiple modalities.<br>
(2) We develop the variational training scheme where the generation model, inference model, and energy-based prior can be jointly and effectively learned.<br>
(3) We conduct various experiments and ablation studies and demonstrate superior performance compared to Laplacian prior baselines.</p>
    <hr>

    <h2>Settings</h2>
    <p>1. Base version: EBM prior.<br>
       2. Generalized version: EBM prior with modality specific prior. </p>
    <hr>
    

    <h2>Code</h2>
    <p>The code can be obtained <a href="https://github.com/syyuan2021/Learning-Multimodal-Latent-Generative-Models-with-EBM">here</a>. </p>
    <hr>
    
    <h2>Experiments</h2>
    <h3>Experiment 1.1: EBM prior Base version on MNIST-SVHN: Digit Coherence</h3>
    <p>Our proposed multimodal generative model with EBM prior generate image with highly consistant digit information both in clean and noisy background. The qualitative results are shown in Figure 1. We further evaluate our model quantitatively by using Joint Coherence and Cross Coherence in the table below. It can be seen that our model achieves superior generation performance compared to listed baseline models.</p>
    <div class="figure">
        <img src="./fig/image/MS_gen_samples_ebm_0_122.png" width="200" style="margin:10px">
        <img src="./fig/image/MS_gen_samples_ebm_0_124.png" width="200" style="margin:10px">
        <img src="./fig/image/MS_gen_samples_ebm_0_127.png" width="200" style="margin:10px">
        <img src="./fig/image/MS_gen_samples_ebm_1_122.png" width="200" style="margin:10px">
        <img src="./fig/image/MS_gen_samples_ebm_1_124.png" width="200" style="margin:10px">
        <img src="./fig/image/MS_gen_samples_ebm_1_127.png" width="200" style="margin:10px">
        <p class="caption"><b>Figure 1:</b> Joint Generated samples for MINIST-SVHN .</p>
    </div>


    <div class="figure">
        <img src="./fig/coherence/ms_base.png" width="600" style="margin:10px">
    </div>


    <h3>Experiment 1.2: EBM prior Base version on PolyMNIST: Digit Coherence</h3>
    <p>  </p>
    <div class="figure">
        <img src="./fig/image/poly_base_joint.png" width="300" style="margin:10px">
        <img src="./fig/image/poly_joint_base_uni.png" width="300" style="margin:10px">
        <p class="caption"><b>Figure 2:</b> Joint Generated samples for PolyMNIST (EBM prior: left; Laplacian prior: right).</p>
    </div>

    <div class="figure">
        <img src="./fig/coherence/poly_base_v1.png" width="400" style="margin:10px">
    </div>

    <h3>Experiment 2: EBM prior Generalized version on PolyMNIST: Digit Coherence</h3>
    <p>  </p>
    <div class="figure">
        <img src="./fig/image/poly_generliazed_joint.png" width="300" style="margin:10px">
        <img src="./fig/image/poly_joint_iclr23.png" width="300" style="margin:10px">
        <p class="caption"><b>Figure 3:</b> Joint Generated samples for PolyMNIST (EBM prior: left; Laplacian prior: right).</p>
    </div>
    
    <div class="figure">
        <img src="./fig/coherence/poly_v2.png" width="400" style="margin:10px">
    </div>


    <h3>Experiment 3: Markov chain transition from standard prior to EBM prior on CUB</h3>
    <p> We examine the exponential tilting of the reference prior \(p_0(z)\) through Langevin samples initialized from \(p_0(z)\) with target distribution \(p_\alpha(z)\). As the reference distribution \(p_0(z)\) is in the form of an Laplacian, we expect the energy-based correction \(f_\alpha\) to tilt \(p_0\) into an irregular shape like some shallow local modes. Therefore, the trajectory of a Markov chain initialized from the reference distribution \(p_0(z)\) with well-learned target \(p_\alpha(z)\) should depict the transition towards more coherent information between image and caption. Figure 4 depicts such transitions for CUB, which is based on a model trained with 50 steps. The quality of synthesis improves significantly with increasing number of steps.
 </p>
    <div class="figure">
        <img src="./fig/image/cub_mcmc.png" width="800" style="margin:10px">
        <p class="caption"><b>Figure 4:</b> Transition of Markov chains initialized from \(p_0(z)\) towards \(\tilde{p}_{\alpha}(z)\) for langevin steps with 50.</p>
    </div>


   
</div>
</body>
</html>
